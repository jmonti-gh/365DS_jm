{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. A Loan Data Practical Example with NumPy\n",
    "- Ejemplo práctico de datos de préstamos con NumPy\n",
    "## 9_03 Setting Up: Checking for Incomplete Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick glance at loan-data.csv (notepad++)\n",
    "- Contains both text and numeric data.\n",
    "- A header clarifying the contents of each column\n",
    "- 1st col is called 'id' -> each row consists of info for the account of a loan candidate's application and each candidate is described by their id. => We referred to the rows as accounts, candidates or applications.\n",
    "- Can't see whether there are missing values in the dtset.\n",
    "- ';' as delimiter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### genfromtxt(autostrip=False)\n",
    "- autostrip: bool, optional\n",
    "Whether to automatically strip white spaces from the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.__version__\n",
    "np.set_printoptions(suppress=True, linewidth=100, precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function show_attr\n",
    "\n",
    "def show_attr(arrnm: str) -> str:\n",
    "    strout = f' {arrnm}: '\n",
    "\n",
    "    for attr in ('shape', 'ndim', 'size', 'dtype'):     #, 'itemsize'):\n",
    "            arrnm_attr = arrnm + '.' + attr\n",
    "            strout += f'| {attr}: {eval(arrnm_attr)} '\n",
    "\n",
    "    return strout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[48010226.  ,         nan,    35000.  , ...,         nan,         nan,     9452.96],\n",
       "       [57693261.  ,         nan,    30000.  , ...,         nan,         nan,     4679.7 ],\n",
       "       [59432726.  ,         nan,    15000.  , ...,         nan,         nan,     1969.83],\n",
       "       ...,\n",
       "       [50415990.  ,         nan,    10000.  , ...,         nan,         nan,     2185.64],\n",
       "       [46154151.  ,         nan,         nan, ...,         nan,         nan,     3199.4 ],\n",
       "       [66055249.  ,         nan,    10000.  , ...,         nan,         nan,      301.9 ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "' raw_data_np: | shape: (10000, 14) | ndim: 2 | size: 140000 | dtype: float64 '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of NANs: 88005\n"
     ]
    }
   ],
   "source": [
    "raw_data_np = np.genfromtxt('9_02_loan-data.csv',\n",
    "                            delimiter=';',\n",
    "                            skip_header=1,\n",
    "                            autostrip=True)\n",
    "display(raw_data_np)\n",
    "display(show_attr('raw_data_np'))\n",
    "print('Num of NANs:', np.isnan(raw_data_np).sum())\n",
    "\n",
    "# Lot of NANs, either text or missing\n",
    "# The entire 1st row is NAN so the skip_header=1\n",
    "# autostrip cause it removes white spaces which can distort our cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of the working process\n",
    "- Gathering (Recopilación de información), Cleaning and Preprocessing the Data <- Data Analysts\n",
    "- DAnalysts hand them over to the DScientists (ML knowlegde) to construct complex Predictive Models.\n",
    "### DAnalysts Rol:\n",
    "1. Our goal is to obtain a clean and preprocessed dataset\n",
    "2. We'll note down all the changes we're making to the original dataset in a documentation file where we describe what each column of the new dtset represents.\n",
    "3. This info will be invaluable to the DScientists who will work with this data after us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A day in the life of a DAnalyst\n",
    "- Explain our role in the project.\n",
    "- Examine the data.\n",
    "- Import the data.\n",
    "- Split the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Case\n",
    "- Rol: DAnalyst in a data science team of central bank in Europe.\n",
    "- Team assignment: create a CRM which estimates the probability of default for every personal account.\n",
    "- Terms like Probability of default, Recovery rate, and Credit Risk Modeling.\n",
    "- Chore: Take the raw dataset and prepare it for the models the plan to run.\n",
    "- Details provided:\n",
    "    1. What data is stored in every column.\n",
    "    2. Set of rules on how to clean and pre-process the values in each column col.\n",
    "> The essence of the DAnalyst job and is much more demanding and sizable than it might initially sound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by Step approach to the problem\n",
    "1. Loan data is a sample from a larger dtset that belongs to an affiliate bank based in USA. Therefore all the values are in dollars, so we need to provide their Euro equivalents.\n",
    "2. Every categorical variable must be quantified. We nee to change any text columns into numbers based on the info they contain.\n",
    "    - Issue date (fecha de emisión) on each loan: transformation is straightforward since we can split the accounts by months.\n",
    "    - For other cols, we only care if they provide positive or negative connotations. So we'll be turning them into __*dummy variables*__ that hold either zero or one.\n",
    "3. Missing Data:\n",
    "    - Furthermore when we're measuring creditworthiness we need to be extremely risk-averse and distrustful of any unavailable data.\n",
    "    - That's why the consensus in the field is that missing info suggest foul play because loan applications are self reported. To elaborate since candidates fill out their loan applications manually, there is an incentive to withhold info which can lower their chances of getting a loan.\n",
    "    - Of course we prefer to give out loans to applicants who can repay them. So __*if the information isn´t available, we'll just assume the worst*__.\n",
    "    - What is worst varies from one column to the next, so the team has provided us with casting directions for each variable in the dtset.\n",
    "    - Therefore as we go through the dtset we'll usually know whether we want to use the minimum, maximum, or some other value when taking care of missing data\n",
    "\n",
    "> Loan info is store in a .csv file called loan-data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translation about Missing Data\n",
    "Además, cuando medimos la solvencia crediticia, debemos ser extremadamente reacios al riesgo y desconfiar de cualquier dato no disponible. Por eso, el consenso en el campo es que la información faltante sugiere un juego sucio porque las solicitudes de préstamos son auto-reportadas. Para explicarlo mejor, dado que los candidatos completan sus solicitudes de préstamo manualmente, existe un incentivo para retener información, lo que puede reducir sus posibilidades de obtener un préstamo. Por supuesto, preferimos otorgar préstamos a los solicitantes que pueden devolverlos. Entonces, si la información no está disponible, simplemente asumiremos lo peor. Lo peor varía de una columna a la siguiente, por lo que el equipo nos ha proporcionado instrucciones de conversión para cada variable en el conjunto de datos. Por lo tanto, a medida que avanzamos en el conjunto de datos, generalmente sabremos si queremos usar el mínimo, el máximo o algún otro valor al ocuparnos de los datos faltantes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
